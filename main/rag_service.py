import os
import asyncio
import threading
import requests
import tempfile
import re
import zipfile
from typing import Optional

from django.conf import settings

# ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã®å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã®è¨­å®š
os.environ["LANGCHAIN_TRACING_V2"] = "false"
os.environ["LANGCHAIN_CALLBACKS_MANAGER"] = "disabled"

from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain.retrievers.multi_query import MultiQueryRetriever


_rag_lock = threading.Lock()
_rag_ready = False
_rag_chain = None


def _extract_google_drive_file_id(url: str) -> Optional[str]:
    """Google Driveã®URLã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’æŠ½å‡º"""
    patterns = [
        r"/file/d/([a-zA-Z0-9-_]+)",
        r"/d/([a-zA-Z0-9-_]+)",
        r"id=([a-zA-Z0-9-_]+)",
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return None


def _download_from_google_drive(file_id: str, local_path: str) -> bool:
    """Google Driveã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå¤§å®¹é‡ç¢ºèªãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾å¿œï¼‰"""
    try:
        print(f"ğŸ“¥ Google Driveã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {file_id}")
        session = requests.Session()

        def _perform_download(token: Optional[str] = None):
            params = {"export": "download", "id": file_id}
            if token:
                params["confirm"] = token
            return session.get("https://drive.google.com/uc", params=params, stream=True)

        # 1å›ç›®ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ãªã—ï¼‰
        response = _perform_download()
        response.raise_for_status()

        # Cookieã«download_warningãŒã‚ã‚Œã°ã€ãã®å€¤ã§å†ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        token = None
        for k, v in response.cookies.items():
            if k.startswith("download_warning"):
                token = v
                break
        if token:
            response = _perform_download(token)
            response.raise_for_status()

        # HTMLãŒè¿”ã£ã¦ãã¦ã„ãªã„ã‹ç°¡æ˜“ãƒã‚§ãƒƒã‚¯
        content_type = response.headers.get("Content-Type", "")
        if "text/html" in content_type.lower():
            print("âŒ å–å¾—ã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯HTMLã§ã™ã€‚å…±æœ‰è¨­å®šã‚„ãƒªãƒ³ã‚¯å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return False

        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        with open(local_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=1024 * 1024):
                if chunk:
                    f.write(chunk)

        print(f"âœ… Google Driveã‹ã‚‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {local_path}")
        return True
    except Exception as e:
        print(f"âŒ Google Driveã‹ã‚‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
        return False


def _download_vectorstore_from_url(url: str, local_path: str) -> bool:
    """å¤–éƒ¨URLã‹ã‚‰ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        print(f"ğŸ“¥ ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {url}")

        # Google Driveã®URLã‹ãƒã‚§ãƒƒã‚¯
        if "drive.google.com" in url:
            file_id = _extract_google_drive_file_id(url)
            if not file_id:
                print("âŒ Google Driveã®URLã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return False
            return _download_from_google_drive(file_id, local_path)

        # é€šå¸¸ã®HTTPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        response = requests.get(url, stream=True)
        response.raise_for_status()

        content_type = response.headers.get("Content-Type", "")
        if "text/html" in content_type.lower():
            print("âŒ å–å¾—ã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯HTMLã§ã™ã€‚URLã‚’ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªã‚‚ã®ã«ã—ã¦ãã ã•ã„ã€‚")
            return False

        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        with open(local_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=1024 * 1024):
                if chunk:
                    f.write(chunk)

        print(f"âœ… ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {local_path}")
        return True
    except Exception as e:
        print(f"âŒ ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
        return False


"""def _default_vector_store_path() -> str:
    # settings.pyã‹ã‚‰ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®ãƒ‘ã‚¹ã‚’å–å¾—
    vector_store_path = getattr(settings, "VECTOR_STORE_PATH", None)
    if not vector_store_path:
        raise ValueError("VECTOR_STORE_PATHãŒsettings.pyã§è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    return str(vector_store_path)""


def _post_extract_fixups(vector_store_path: str):
    """å±•é–‹å¾Œã«æƒ³å®šå¤–ã®é…ç½®ã ã£ãŸå ´åˆã®è£œæ­£ã€‚ãƒˆãƒƒãƒ—ã«ç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆãªã©ã€‚"""
    parent_dir = os.path.dirname(vector_store_path)
    index_f = os.path.join(parent_dir, "index.faiss")
    index_p = os.path.join(parent_dir, "index.pkl")
    if not os.path.exists(vector_store_path) and (os.path.exists(index_f) or os.path.exists(index_p)):
        os.makedirs(vector_store_path, exist_ok=True)
        if os.path.exists(index_f):
            os.replace(index_f, os.path.join(vector_store_path, "index.faiss"))
        if os.path.exists(index_p):
            os.replace(index_p, os.path.join(vector_store_path, "index.pkl"))
        print("â„¹ï¸ ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦å®šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ã—ã¾ã—ãŸ")


def _ensure_vectorstore_exists():
    """ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ãŒå­˜åœ¨ã—ãªã„å ´åˆã€å¤–éƒ¨ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    vector_store_path = _default_vector_store_path()

    if not os.path.exists(vector_store_path):
        vectorstore_url = os.getenv("VECTORSTORE_URL")
        if vectorstore_url:
            with tempfile.NamedTemporaryFile(suffix=".zip", delete=False) as tmp_file:
                downloaded = _download_vectorstore_from_url(vectorstore_url, tmp_file.name)
            if not downloaded:
                print("âŒ ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚URLã¨å…±æœ‰è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                return

            # ZIPåˆ¤å®š
            if not zipfile.is_zipfile(tmp_file.name):
                try:
                    # å…ˆé ­æ•°ãƒã‚¤ãƒˆã‚’è¡¨ç¤ºã—ã¦ãƒ‡ãƒãƒƒã‚°ã«å½¹ç«‹ã¦ã‚‹
                    with open(tmp_file.name, "rb") as f:
                        head = f.read(64)
                    print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯ZIPã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆé ­ãƒã‚¤ãƒˆ: {head[:16]!r}")
                finally:
                    os.unlink(tmp_file.name)
                return

            # å±•é–‹
            try:
                with zipfile.ZipFile(tmp_file.name, "r") as zip_ref:
                    zip_ref.extractall(os.path.dirname(vector_store_path))
                print(f"âœ… ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’å±•é–‹ã—ã¾ã—ãŸ: {vector_store_path}")
            finally:
                os.unlink(tmp_file.name)

            # å±•é–‹å¾Œè£œæ­£
            _post_extract_fixups(vector_store_path)
        else:
            print("âš ï¸ VECTORSTORE_URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")


def _build_rag_chain():
    google_api_key = getattr(settings, "GOOGLE_API_KEY", None)
    if not google_api_key:
        raise RuntimeError("GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®å­˜åœ¨ç¢ºèªã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    _ensure_vectorstore_exists()

    vector_store_path = _default_vector_store_path()

    print(f"ğŸ” ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ãƒ‘ã‚¹ç¢ºèª: {vector_store_path}")
    if not os.path.exists(vector_store_path):
        raise FileNotFoundError(f"ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {vector_store_path}")

    # æ–°ã—ã„ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’è¨­å®šï¼ˆåˆæœŸåŒ–æ™‚ã®ã¿ï¼‰
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    # Embeddings ã¨ VectorStore ã‚’åˆæœŸåŒ–
    print("ğŸ“¥ Embeddingsãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=google_api_key)

    print("ğŸ“‚ ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢èª­ã¿è¾¼ã¿ä¸­...")
    vector_store = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)

    base_retriever = vector_store.as_retriever(
        search_type="similarity_score_threshold", search_kwargs={"k": 16, "score_threshold": 0.1}
    )

    print("ğŸ¤– LLMãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash-lite",
        google_api_key=google_api_key,
        temperature=0,
        max_retries=1,
        timeout=60,
    )

    prompt_template = (
        """
ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ã€Œå‚è€ƒæƒ…å ±ã€ã‚’ä¸»ã«ç”¨ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œè³ªå•ã€ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€æŒ™ã’ã‚‹ä½œå“ã¯ã§ãã‚‹é™ã‚Šå°èª¬ã§ç‹‚è¨€ã‚„èƒ½ã¯é¿ã‘ã‚‹ã‚ˆã†ã«ã—ã€1ã¤ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯ä»¥ä¸‹ã«æº–æ‹ ã—ã¦ãã ã•ã„ã€‚
ãŸã ã—ã€å…·ä½“çš„ãªä½œè€…ã‚’æŒ‡å®šã•ã‚Œãªã„å ´åˆã€ä½œå“ãŒèª°ã‚‚ãŒèª­ã‚“ã ã“ã¨ã®ã‚ã‚‹ã‚ˆã†ãªä½œå“ã«ãªã£ã¦ã—ã¾ã†ã“ã¨ã‚’é¿ã‘ã¦ãã ã•ã„ã€‚
å‚è€ƒæƒ…å ±ãŒä¸ååˆ†ãªå ´åˆã¯ã€æ¤œç´¢ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹æƒ…å ±ã§ã‚‚æ§‹ã„ã¾ã›ã‚“ãŒé’ç©ºæ–‡åº«ã®ä¸­ã‹ã‚‰å…·ä½“çš„ãªä½œå“æƒ…å ±ã‚’å›ç­”ã—ã¦ãã ã•ã„ã€‚å›ç­”ã¯ç°¡æ½”ã«ã€å¯èƒ½ãªã‚‰ç®‡æ¡æ›¸ãã§ç¤ºã—ã¦ãã ã•ã„ã€‚æœ€å¾Œã«å‚è€ƒã«ã—ãŸä½œå“åï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ»ä½œè€…ï¼‰ã‚’åˆ—æŒ™ã—ã¦ãã ã•ã„ã€‚


ã€å‚è€ƒæƒ…å ±ã€‘
{context}

ã€è³ªå•ã€‘
{question}

ã€å›ç­”ã€‘
"""
    )
    prompt = ChatPromptTemplate.from_template(prompt_template)

    print("ğŸ” Retrieverè¨­å®šå®Œäº†")

    def format_docs(docs):
        lines = []
        for i, d in enumerate(docs, 1):
            md = getattr(d, "metadata", {}) or {}
            title = md.get("title", "ä¸æ˜")
            author = md.get("author", "ä¸æ˜")
            source = md.get("source", "")
            lines.append(f"[{i}] ã‚¿ã‚¤ãƒˆãƒ«: {title}ï¼ˆ{author}ï¼‰\nå‡ºå…¸: {source}\næœ¬æ–‡æŠœç²‹:\n{getattr(d, 'page_content', '')}")
        return "\n\n---\n\n".join(lines)

    def filter_docs(docs):
        filtered = []
        for d in docs:
            text = (getattr(d, "page_content", "") or "").strip()
            if len(text) < 60:
                continue
            if sum(ch.isdigit() for ch in text) > max(1, int(len(text) * 0.4)):
                continue
            filtered.append(d)
        return filtered or docs

    QUERY_KEYWORDS = [
        "æ˜¥", "æ˜¥é¢¨", "æ˜¥é›¨", "æ˜¥å®µ", "æœ§æœˆå¤œ", "è‹¥è‘‰", "æ–°ç·‘", "èŠ±è¦‹", "æ¡œ", "æ¢…", "èœã®èŠ±", "ã¤ã¤ã˜", "è—¤", "ã†ãã„ã™",
        "å¤", "çœŸå¤", "ç››å¤", "å¤è‡³", "å¤•æš®ã‚Œ", "å¤•æ–¹", "é»„æ˜", "å¤•ç„¼ã‘", "å¤•ç«‹", "ç´æ¶¼", "æ‰“ã¡æ°´", "æš‘æ°—æ‰•ã„",
        "è‰", "ã›ã¿", "è›", "ã»ãŸã‚‹", "é‡‘é­š", "å…¥é“é›²", "æœé¡”", "å‘æ—¥è‘µ", "ã²ã¾ã‚ã‚Š", "ãƒ©ãƒ ãƒ", "æ°·æ°´",
        "æµ·", "æ¸š", "ç ‚æµœ", "æ½®é¢¨", "æµœè¾º", "ç¸æ—¥", "ç¥­", "æµ´è¡£", "å›£æ‰‡", "ã†ã¡ã‚", "æ‰‡å­", "èŠ±ç«", "ç›†", "ç›†è¸Šã‚Š",
        "ç§‹", "ç§‹é›¨", "ç§‹é¢¨", "é‡åˆ†", "å¤œé•·", "å½¼å²¸", "å½¼å²¸èŠ±", "æ›¼ç æ²™è¯", "æœˆ", "åæœˆ", "ä¸­ç§‹", "æœˆè¦‹",
        "ç´…è‘‰", "é»„è‘‰", "ç¨²ç©‚", "æ–°ç±³", "ã™ã™ã", "è™«ã®éŸ³", "éˆ´è™«", "æ¾è™«", "ãã‚Šãã‚Šã™", "éŠ€æ",
        "å†¬", "çœŸå†¬", "æœ¨æ¯ã—", "åŒ—é¢¨", "éœœ", "éœœå¤œ", "é›ª", "ç²‰é›ª", "åˆé›ª", "å¹é›ª", "æ°·", "å‡ã¦ã¤ã",
        "ç‚¬ç‡µ", "ã“ãŸã¤", "ç«é‰¢", "ç‚­ç«", "å›²ç‚‰è£", "ã¿ã‹ã‚“", "é›ªæ˜ã‚Š", "å¸«èµ°", "å¹´ã®ç€¬",
        "é›¨", "å°é›¨", "é©Ÿé›¨", "ã«ã‚ã‹é›¨", "éœ§", "éœ", "é›²", "æ›‡", "å¿«æ™´", "æ™´å¤©", "é›·", "ç¨²å¦»", "è™¹",
        "é¢¨", "æ¶¼é¢¨", "å¯’é¢¨", "å—é¢¨", "åŒ—é¢¨",
        "æœ", "æœç„¼ã‘", "æ˜¼", "æ­£åˆ", "å¤•", "é€¢é­”ãŒæ™‚", "å®µ", "å®µé—‡", "å¤œ", "çœŸå¤œä¸­", "å¤œæ˜ã‘", "æš", "æ˜ã‘æ–¹",
    ]

    def extract_query_terms(question: str) -> set:
        terms = set()
        if not question:
            return terms
        for w in QUERY_KEYWORDS:
            if w in question:
                terms.add(w)
        return terms

    def rerank_docs(docs, question: str, top_n: int = 10):
        docs = filter_docs(docs)
        terms = extract_query_terms(question)
        if not terms:
            return docs[:top_n]

        filtered_by_terms = []
        for d in docs:
            text = getattr(d, "page_content", "")
            md = getattr(d, "metadata", {}) or {}
            kw = md.get("keywords") or {}
            hit = any(t in text for t in terms)
            if not hit and isinstance(kw, dict):
                for arr in kw.values():
                    if any(t in arr for t in terms):
                        hit = True
                        break
            if hit:
                filtered_by_terms.append(d)
        if filtered_by_terms:
            docs = filtered_by_terms

        def score(d):
            text = getattr(d, "page_content", "")
            s = 0
            for t in terms:
                s += text.count(t) * 2
            md = getattr(d, "metadata", {}) or {}
            kw = md.get("keywords", {}) or {}
            if isinstance(kw, dict):
                for arr in kw.values():
                    for t in terms:
                        if t in arr:
                            s += 3
            return s

        ranked = sorted(docs, key=score, reverse=True)
        return ranked[:top_n]

    print("âš™ï¸ RAGãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰ä¸­...")

    def rag_pipeline(question):
        docs = base_retriever.invoke(question)
        filtered_docs = rerank_docs(docs, question)
        context = format_docs(filtered_docs)
        messages = prompt.format_messages(context=context, question=question)
        response = llm.invoke(messages)
        return response.content

    print("âœ… RAGãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰å®Œäº†")
    return rag_pipeline


def ensure_rag_ready():
    global _rag_ready, _rag_chain
    if _rag_ready and _rag_chain is not None:
        return
    with _rag_lock:
        if not _rag_ready:
            print("ğŸš€ RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹...")
            try:
                _rag_chain = _build_rag_chain()
                _rag_ready = True
                print("âœ… RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            except Exception as e:
                print(f"âŒ RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
                raise


def _run_with_event_loop(func, *args, **kwargs):
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(_run_in_new_thread, func, *args, **kwargs)
                return future.result()
        else:
            return func(*args, **kwargs)
    except RuntimeError:
        return _run_in_new_thread(func, *args, **kwargs)


def _run_in_new_thread(func, *args, **kwargs):
    def run_with_new_loop():
        new_loop = asyncio.new_event_loop()
        asyncio.set_event_loop(new_loop)
        try:
            return func(*args, **kwargs)
        finally:
            new_loop.close()

    import concurrent.futures
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future = executor.submit(run_with_new_loop)
        return future.result()


def ask(question: str) -> str:
    ensure_rag_ready()
    try:
        print(f"ğŸ” RAGè³ªå•å‡¦ç†é–‹å§‹: {question}")
        result = _rag_chain(question)
        print(f"âœ… RAGå›ç­”ç”Ÿæˆå®Œäº†: {len(result)} æ–‡å­—")
        return result
    except RuntimeError as e:
        if "event loop" in str(e).lower():
            print(f"âš ï¸ ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼ã‚’æ¤œå‡ºã€ä»£æ›¿å®Ÿè¡Œä¸­: {e}")
            return _run_with_event_loop(_rag_chain, question)
        else:
            raise


